# -*- coding: utf-8 -*-
"""Conversational Insight Platform.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4G8Zy-K7WblL4QcPWw31B9sw6TSyXng
"""

!pip install streamlit

!wget -q -O - ipv4.icanhazip.com

!pip install bertopic
!pip install moviepy
!pip install transformers
!pip install spacy
!pip install nltk
!pip install plotly
!pip install matplotlib
!pip install ffmpeg-python

# NLTK setup
import nltk
nltk.download('vader_lexicon')

!pip install git+https://github.com/openai/whisper.git

!pip install vaderSentiment

!pip install transformers

!pip install gensim

from transformers import pipeline

!pip install keybert

!pip install librosa

!pip install transformers
!huggingface-cli login

!pip install googletrans==4.0.0-rc1

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sample.py
# import os
# import streamlit as st
# import whisper
# import moviepy.editor as mp
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import nltk
# import librosa
# import gensim
# from gensim import corpora
# from nltk.corpus import stopwords
# from keybert import KeyBERT
# 
# nltk.download('vader_lexicon')
# 
# model = whisper.load_model("base")
# 
# def extract_audio_from_video(video_file):
#     clip = mp.VideoFileClip(video_file)
#     audio_path = video_file.replace('.mp4', '.wav')
#     clip.audio.write_audiofile(audio_path)
#     return audio_path
# 
# def transcribe_audio(file_path):
#     if file_path.endswith('.mp4'):
#         file_path = extract_audio_from_video(file_path)
#     result = model.transcribe(file_path)
#     return result['text']
# 
# import nltk
# import gensim
# from gensim import corpora
# from nltk.corpus import stopwords
# from keybert import KeyBERT
# 
# keyword_extractor = KeyBERT()
# 
# nltk.download('stopwords')
# stop_words = stopwords.words('english')
# 
# def preprocess_text(text):
#     tokens = gensim.utils.simple_preprocess(text, deacc=True)
#     return [word for word in tokens if word not in stop_words]
# 
# 
# def extract_topics_lda(transcription, num_topics=5):
#     processed_text = preprocess_text(transcription)
# 
#     dictionary = corpora.Dictionary([processed_text])
#     corpus = [dictionary.doc2bow(processed_text)]
# 
#     lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)
# 
#     # Get the topics
#     topics = lda_model.print_topics(num_words=4)  # Get top 4 words for each topic
#     return topics
# 
# 
# def extract_metadata(file_path, transcription):
#     # Duration of the audio
#     y, sr = librosa.load(file_path, sr=None)
#     duration = librosa.get_duration(y=y, sr=sr)
# 
#     # Word count
#     word_count = len(transcription.split())
# 
#     # Extract keywords
#     keywords = keyword_extractor.extract_keywords(transcription, keyphrase_ngram_range=(1, 2), stop_words='english')
# 
#     return duration, word_count, keywords
# 
# 
# # Load a translation pipeline
# 
# from googletrans import Translator
# 
# # Initialize the Google Translate API
# translator = Translator()
# 
# # Function to translate using Google Translate
# def translate_google_transcript(transcription, target_language_code):
#     translation = translator.translate(transcription, dest=target_language_code)
#     return translation.text
# 
# def analyze_sentiments(transcription):
#     sia = SentimentIntensityAnalyzer()
#     sentiment_scores = sia.polarity_scores(transcription)
#     sentiments = {
#         "positive": sentiment_scores['pos'],
#         "neutral": sentiment_scores['neu'],
#         "negative": sentiment_scores['neg']
#     }
#     return sentiments
# 
# from transformers import pipeline
# 
# def answer_query(transcription, query):
#     # Use a question-answering model from Hugging Face
#     query_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")
# 
#     # Limit the length of the transcription (context) passed to the model
#     max_context_length = 512  # Typical length limit for many transformer models
# 
#     # If transcription is too long, split it into chunks (sentences or paragraphs)
#     if len(transcription.split()) > max_context_length:
#         # Optionally, break down the transcription into more manageable chunks
#         context_chunks = [transcription[i:i+max_context_length] for i in range(0, len(transcription.split()), max_context_length)]
# 
#         # Answering the query on each chunk, and aggregating answers
#         answers = []
#         for chunk in context_chunks:
#             result = query_pipeline(question=query, context=chunk)
#             answers.append(result['answer'])
# 
#         # Combine all answers from different chunks (simple concatenation here)
#         final_answer = " ".join(answers)
#     else:
#         # If the context is short enough, just run the pipeline on the full transcription
#         result = query_pipeline(question=query, context=transcription)
#         final_answer = result['answer']
# 
#     return final_answer
# 
# 
# from transformers import pipeline
# 
# def generate_insights(transcription):
#     # Load a summarization model from Hugging Face
#     summarization_pipeline = pipeline("summarization", model="facebook/bart-large-cnn")
# 
#     # Generate the summary from the transcription
#     insights = summarization_pipeline(transcription, max_length=150, min_length=40, do_sample=False)
# 
#     # Return the summarized text
#     return insights[0]['summary_text']
# 
# 
# 
# # Streamlit app
# import streamlit as st
# st.header('CONVERSATIONAL INSIGHT PLATFORM')
# 
# 
# uploaded_file = st.file_uploader("Choose an audio or video file of any type", type=["mp3", "wav", "mp4"])
# 
# if uploaded_file is not None:
#     file_path = os.path.join('/tmp', uploaded_file.name)
#     with open(file_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
# 
#     # Step 1: Transcribe the file and get the full result
#     st.subheader("Transcription")
#     transcription = transcribe_audio(file_path)  # This returns the full Whisper result
#  # Extract just the text from the result
#     string = st.text_area(transcription, height=300)
#     st.write(string)
# 
# 
#     # Step 2: Extract topics from the transcription
#     st.subheader("Extracted Topics")
#     topics = extract_topics_lda(transcription)
#     for i, topic in enumerate(topics):
#         st.write(f"Topic {i+1}: {topic}")
# 
# 
#     if transcription:
#         st.subheader("Metadata Extraxtion")
#         duration, word_count, keywords = extract_metadata(file_path, transcription)
#         st.write(f"Duration: {duration:.2f} seconds")
#         st.write(f"Word Count: {word_count}")
#         st.write("Keywords: ", [kw[0] for kw in keywords])
# 
#     st.subheader("Sentiment Analysis")
#     sentiments = analyze_sentiments(transcription)
#     st.write(f"Positive: {sentiments['positive']}")
#     st.write(f"Neutral: {sentiments['neutral']}")
#     st.write(f"Negative: {sentiments['negative']}")
# 
#     st.subheader("Generate Insights")
#     insights = generate_insights(transcription)
#     st.write(insights)
# 
#     st.subheader("Ask a Question About the Conversation")
#     user_query = st.text_input("Enter your query")
#     if user_query:
#       answer = answer_query(transcription, user_query)
#       st.write(f"Answer: {answer}")
# 
#     st.subheader("Translate Transcription")
#     languages = {
#     "French": "fr",
#     "German": "de",
#     "Spanish": "es",
#     "Tamil": "ta",
#     "Telugu": "te"
#     }
#     selected_language = st.selectbox("Select the language to translate to:", list(languages.keys()))
# 
#     if selected_language:
#       target_lang_code = languages[selected_language]
#       translated_text = translate_google_transcript(transcription, target_lang_code)
#       st.subheader(f"Transcription Translated to {selected_language}")
#       st.write(translated_text)

"""
 **Run Streamlit**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sample_1.py
# import os
# import streamlit as st
# import whisper
# import moviepy.editor as mp
# from bertopic import BERTopic
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import nltk
# import librosa
# import gensim
# from gensim import corpora
# from nltk.corpus import stopwords
# from keybert import KeyBERT
# import matplotlib.pyplot as plt
# import seaborn as sns
# from googletrans import Translator
# from transformers import pipeline
# 
# # Download NLTK data
# nltk.download('vader_lexicon')
# nltk.download('stopwords')
# 
# # Initialize Whisper model for ASR
# model = whisper.load_model("base")
# 
# # Initialize the keyword extraction model
# keyword_extractor = KeyBERT()
# stop_words = stopwords.words('english')
# 
# # Initialize the Google Translate API
# translator = Translator()
# 
# # Helper functions
# def extract_audio_from_video(video_file):
#     clip = mp.VideoFileClip(video_file)
#     audio_path = video_file.replace('.mp4', '.wav')
#     clip.audio.write_audiofile(audio_path)
#     return audio_path
# 
# def transcribe_audio(file_path):
#     if file_path.endswith('.mp4'):
#         file_path = extract_audio_from_video(file_path)
#     result = model.transcribe(file_path)
#     return result
# 
# def preprocess_text(text):
#     tokens = gensim.utils.simple_preprocess(text, deacc=True)  # Tokenizes and removes punctuation
#     return [word for word in tokens if word not in stop_words]
# 
# def calculate_transcription_confidence(whisper_output):
#     segments = whisper_output.get('segments', [])
#     confidences = [segment['avg_logprob'] for segment in segments]
#     avg_confidence = sum(confidences) / len(confidences) if confidences else 0
#     confidence_percentage = (1 + avg_confidence) * 50  # Convert log probability to a percentage
#     return confidence_percentage
# 
# def extract_topics_lda(transcription, num_topics=5):
#     processed_text = preprocess_text(transcription)
#     dictionary = corpora.Dictionary([processed_text])
#     corpus = [dictionary.doc2bow(processed_text)]
#     lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)
#     topics = lda_model.print_topics(num_words=4)
#     return topics, lda_model, corpus, dictionary, processed_text
# 
# def extract_metadata(file_path, transcription):
#     y, sr = librosa.load(file_path, sr=None)
#     duration = librosa.get_duration(y=y, sr=sr)
#     word_count = len(transcription.split())
#     keywords = keyword_extractor.extract_keywords(transcription, keyphrase_ngram_range=(1, 2), stop_words='english')
#     return duration, word_count, keywords
# 
# def translate_google_transcript(transcription, target_language_code):
#     translation = translator.translate(transcription, dest=target_language_code)
#     return translation.text
# 
# def analyze_sentiments(transcription):
#     sia = SentimentIntensityAnalyzer()
#     sentiment_scores = sia.polarity_scores(transcription)
#     sentiments = {
#         "positive": sentiment_scores['pos'],
#         "neutral": sentiment_scores['neu'],
#         "negative": sentiment_scores['neg']
#     }
#     return sentiments
# 
# def answer_query(transcription, query):
#     query_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")
#     max_context_length = 512
#     if len(transcription.split()) > max_context_length:
#         context_chunks = [transcription[i:i+max_context_length] for i in range(0, len(transcription.split()), max_context_length)]
#         answers = []
#         for chunk in context_chunks:
#             result = query_pipeline(question=query, context=chunk)
#             answers.append(result['answer'])
#         final_answer = " ".join(answers)
#     else:
#         result = query_pipeline(question=query, context=transcription)
#         final_answer = result['answer']
#     return final_answer
# 
# def generate_insights(transcription):
#     summarization_pipeline = pipeline("summarization", model="facebook/bart-large-cnn")
#     insights = summarization_pipeline(transcription, max_length=150, min_length=40, do_sample=False)
#     return insights[0]['summary_text']
# 
# # Plotting functions
# def plot_confidence_scores(confidence_scores):
#     plt.figure(figsize=(10, 6))
#     sns.histplot(confidence_scores, kde=True)
#     plt.title('Distribution of Transcription Confidence Scores')
#     plt.xlabel('Confidence Score (%)')
#     plt.ylabel('Frequency')
#     st.pyplot()
# 
# def plot_topic_coherence_scores(coherence_scores):
#     topics = [f"Topic {i+1}" for i in range(len(coherence_scores))]
#     plt.figure(figsize=(10, 6))
#     plt.bar(topics, coherence_scores, color='lightcoral')
#     plt.title('Topic Coherence Scores')
#     plt.xlabel('Topics')
#     plt.ylabel('Coherence Score')
#     plt.ylim(0, 1)
#     st.pyplot()
# 
# def plot_sentiment_distribution(sentiments):
#     labels = list(sentiments.keys())
#     sizes = list(sentiments.values())
#     plt.figure(figsize=(10, 6))
#     plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['#ff9999','#66b3ff','#99ff99'])
#     plt.title('Sentiment Distribution')
#     st.pyplot()
# 
# # Streamlit app
# st.header('CONVERSATIONAL INSIGHT PLATFORM')
# 
# uploaded_file = st.file_uploader("Choose an audio or video file of any type", type=["mp3", "wav", "mp4"])
# 
# if uploaded_file is not None:
#     file_path = os.path.join('/tmp', uploaded_file.name)
#     with open(file_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
# 
#     # Step 1: Transcribe the file and get the full result
#     st.subheader("Transcription")
#     transcription_result = transcribe_audio(file_path)
#     transcription = transcription_result['text']  # Extract just the text from the result
# 
#     # Display transcription
#     string = st.text_area("Transcription", transcription, height=300)
# 
#     # Step 2: Calculate and display transcription confidence score
#     transcription_confidence = calculate_transcription_confidence(transcription_result)
#     st.write(f"Transcription Confidence: {transcription_confidence:.2f}%")
# 
#     # Plot the transcription confidence scores
#     plot_confidence_scores([transcription_confidence])
# 
#     # Step 3: Extract topics from the transcription
#     st.subheader("Extracted Topics")
#     topics, lda_model, corpus, dictionary, processed_text = extract_topics_lda(transcription)
#     coherence_scores = [calculate_topic_coherence(lda_model, corpus, dictionary, processed_text)]
#     for i, topic in enumerate(topics):
#         st.write(f"Topic {i+1}: {topic}")
# 
#     # Plot topic coherence scores
#     plot_topic_coherence_scores(coherence_scores)
# 
#     # Step 4: Extract metadata
#     if transcription:
#         st.subheader("Metadata Extraction")
#         duration, word_count, keywords = extract_metadata(file_path, transcription)
#         st.write(f"Duration: {duration:.2f} seconds")
#         st.write(f"Word Count: {word_count}")
#         st.write("Keywords: ", [kw[0] for kw in keywords])
# 
#     # Step 5: Analyze sentiment
#     st.subheader("Sentiment Analysis")
#     sentiments = analyze_sentiments(transcription)
#     st.write(f"Positive: {sentiments['positive']}")
#     st.write(f"Neutral: {sentiments['neutral']}")
#     st.write(f"Negative: {sentiments['negative']}")
# 
#     # Plot sentiment distribution
#     plot_sentiment_distribution(sentiments)
# 
#     # Step 6: Generate insights
#     st.subheader("Generate Insights")
#     insights = generate_insights(transcription)
#     st.write(insights)
# 
#     # Step 7: Generate actionable insights
#     st.subheader("Ask a Question About the Conversation")
#     user_query = st.text_input("Enter your query")
#     if user_query:
#         answer = answer_query(transcription, user_query)
#         st.write(f"Answer: {answer}")
# 
#     # Step 8: Translate transcription
#     st.subheader("Translate Transcription")
#     languages = {
#         "French": "fr",
#         "German": "de",
#         "Spanish": "es",
#         "Tamil": "ta",
#         "Telugu": "te"
#     }
#     selected_language = st.selectbox("Select the language to translate to:", list(languages.keys()))
# 
#     if selected_language:
#         target_lang_code = languages[selected_language]
#         translated_text = translate_google_transcript(transcription, target_lang_code)
#         st.subheader(f"Transcription Translated to {selected_language}")
#         st.write(translated_text)
#

! streamlit run sample_1.py & npx localtunnel --port 8501